{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.utils as vutils\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from data import BraTSDataset\n",
    "\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 999\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset configuration\n",
    "dataset_root = \"dataset\"\n",
    "t1_train_data = \"data/MICCAI_BraTS2020/train/t1\"\n",
    "\n",
    "# Critic model configuration\n",
    "use_gp = True\n",
    "lambda_gp = 10\n",
    "# clipping param won't be used if use_gp is True\n",
    "clipping_param = 0.01\n",
    "\n",
    "# Generator model configuration\n",
    "latent_size = 200\n",
    "\n",
    "# Training configuration\n",
    "device = torch.device(\"cuda:0\")\n",
    "batch_size = 16\n",
    "real_label = -1.0\n",
    "fake_label = 1.0\n",
    "n_epochs = 10\n",
    "n_critic = 5\n",
    "\n",
    "# Dataset configuration\n",
    "image_size = 240\n",
    "num_workers = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = [os.path.join(t1_train_data, impath) for impath in os.listdir(t1_train_data)]\n",
    "tf = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.CenterCrop(image_size),\n",
    "    transforms.ToTensor(),\n",
    "    ])\n",
    "dataset = BraTSDataset(image_paths, tf)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_batch = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot some training images\n",
    "print(real_batch.shape)\n",
    "# Actually plot it\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Training Images\")\n",
    "plt.imshow(\n",
    "    np.transpose(\n",
    "        vutils.make_grid(real_batch.to(device)[:16], padding=2, normalize=True).cpu(),\n",
    "        (1, 2, 0)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_init(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "        nn.init.normal_(m.weight, 0.0, 0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, use_gp=False) -> None:\n",
    "        super(Critic, self).__init__()\n",
    "        if use_gp:\n",
    "            self.main = nn.Sequential(\n",
    "                # input: 1 x 240 x 240\n",
    "                nn.Conv2d(1, 128, 4, stride=2, padding=9),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                # input: 128 x 128 x 128\n",
    "                nn.Conv2d(128, 64, 4, stride=2, padding=1),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                # input: 64 x 64 x 64\n",
    "                nn.Conv2d(64, 32, 4, stride=2, padding=1),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                # input: 32 x 32 x 32\n",
    "                nn.Conv2d(32, 16, 4, stride=2, padding=1),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                # input: 16 x 16 x 16\n",
    "                nn.Conv2d(16, 8, 4, stride=2, padding=1),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                # input: 8 x 8 x 8\n",
    "                nn.Conv2d(8, 4, 4, stride=2, padding=1),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                # input: 4 x 4 x 4\n",
    "                nn.Conv2d(4, 1, 4, stride=1, padding=0),\n",
    "                # input: 1 x 1 x 1\n",
    "            )\n",
    "        else:\n",
    "            self.main = nn.Sequential(\n",
    "                # input: 1 x 240 x 240\n",
    "                nn.Conv2d(1, 128, 4, stride=2, padding=9),\n",
    "                nn.BatchNorm2d(128),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                # input: 128 x 128 x 128\n",
    "                nn.Conv2d(128, 64, 4, stride=2, padding=1),\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                # input: 64 x 64 x 64\n",
    "                nn.Conv2d(64, 32, 4, stride=2, padding=1),\n",
    "                nn.BatchNorm2d(32),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                # input: 32 x 32 x 32\n",
    "                nn.Conv2d(32, 16, 4, stride=2, padding=1),\n",
    "                nn.BatchNorm2d(16),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                # input: 16 x 16 x 16\n",
    "                nn.Conv2d(16, 8, 4, stride=2, padding=1),\n",
    "                nn.BatchNorm2d(8),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                # input: 8 x 8 x 8\n",
    "                nn.Conv2d(8, 4, 4, stride=2, padding=1),\n",
    "                nn.BatchNorm2d(4),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                # input: 4 x 4 x 4\n",
    "                nn.Conv2d(4, 1, 4, stride=1, padding=0),\n",
    "                # input: 1 x 1 x 1\n",
    "            )\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic_model = Critic(use_gp=use_gp).to(device)\n",
    "critic_model.apply(weight_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # input: latent_size x 1 x 1\n",
    "            nn.ConvTranspose2d(latent_size, 128, 4, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            # input: 128 x 4 x 4\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            # input: 64 x 8 x 8\n",
    "            nn.ConvTranspose2d(64, 32, 4, 2, 1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            # input: 32 x 16 x 16\n",
    "            nn.ConvTranspose2d(32, 16, 4, 2, 1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            # input: 16 x 32 x 32\n",
    "            nn.ConvTranspose2d(16, 8, 4, 2, 1),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            # input: 8 x 64 x 64\n",
    "            nn.ConvTranspose2d(8, 4, 4, 2, 1),\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            # input: 4 x 128 x 128\n",
    "            nn.ConvTranspose2d(4, 1, 4, 2, 9),\n",
    "            nn.Tanh()\n",
    "            # output: 1 x 240 x 240\n",
    "        )\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_model = Generator().to(device)\n",
    "generator_model.apply(weight_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_critic = optim.RMSprop(critic_model.parameters(), lr=5e-5)\n",
    "optimizer_generator = optim.RMSprop(generator_model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "print(\"Starting training....\")\n",
    "fixed_noise = torch.randn(16, latent_size, 1, 1, device=device)\n",
    "img_list = list()\n",
    "d_loss_hist = list()\n",
    "g_loss_hist = list()\n",
    "best_g_loss = float(\"inf\")\n",
    "best_g_weights = None\n",
    "best_d_weights = None\n",
    "\n",
    "print(f\"Number of mini batch iterations per epoch: {len(dataloader)}\")\n",
    "for epoch in range(n_epochs):\n",
    "    for i, data in tqdm(enumerate(dataloader, 0)):\n",
    "        # Get batch of data\n",
    "        real_batch = data.to(device)\n",
    "        b_size = real_batch.shape[0]\n",
    "\n",
    "        d_loss_vals = list()\n",
    "        for _ in range(n_critic):\n",
    "            # Train critic weight for real images\n",
    "            critic_model.zero_grad()\n",
    "            d_loss_real = critic_model(real_batch)\n",
    "\n",
    "            # Update critic weight for fake images\n",
    "            noise = torch.randn(b_size, latent_size, 1, 1, device=device)\n",
    "            fake_batch = generator_model(noise)\n",
    "            d_loss_fake = critic_model(fake_batch)\n",
    "\n",
    "            d_loss = -torch.mean(d_loss_real - d_loss_fake)\n",
    "            if use_gp:\n",
    "                # Random weight term for interpolation\n",
    "                alpha = torch.rand(b_size, 1, 1, 1).to(device)\n",
    "                # Get random interpolations between real and fake\n",
    "                interpolates = torch.autograd.Variable(alpha * real_batch + (1 - alpha) * fake_batch, requires_grad=True).to(device)\n",
    "                d_interpolates = critic_model(interpolates)\n",
    "                fake = torch.autograd.Variable(torch.Tensor(d_interpolates.size()).fill_(1.0), requires_grad=False).to(device)\n",
    "                # Get gradient w.r.t interpolates\n",
    "                gradients = torch.autograd.grad(\n",
    "                    outputs=d_interpolates,\n",
    "                    inputs=interpolates,\n",
    "                    grad_outputs=fake,\n",
    "                    create_graph=True,\n",
    "                )[0]\n",
    "                gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "                d_loss += lambda_gp * gradient_penalty\n",
    "            d_loss_vals.append(d_loss.detach().cpu().numpy())\n",
    "            d_loss.backward()\n",
    "            optimizer_critic.step()\n",
    "\n",
    "            if use_gp:\n",
    "                # no op\n",
    "                ...\n",
    "            else:\n",
    "                # Clamp critic weights\n",
    "                for p in critic_model.parameters():\n",
    "                    p.data.clamp_(-clipping_param, clipping_param)\n",
    "\n",
    "        d_loss_hist.append(np.mean(d_loss_vals))\n",
    "\n",
    "        # Update generator weight\n",
    "        generator_model.zero_grad()\n",
    "        noise = torch.randn(b_size, 100, 1, 1, device=device)\n",
    "        fake_batch = generator_model(noise)\n",
    "        output = critic_model(fake_batch)\n",
    "        g_loss = -torch.mean(output)\n",
    "        g_loss_hist.append(g_loss.detach().cpu().numpy())\n",
    "        g_loss.backward()\n",
    "        optimizer_generator.step()\n",
    "\n",
    "        # Summarize performance\n",
    "        if i % 24 == 0:\n",
    "            tqdm.write(f\"epoch: {epoch}, d_loss: {d_loss_hist[-1]:.4f}, g_loss: {g_loss_hist[-1]:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if g_loss_hist[-1] < best_g_loss:\n",
    "            best_g_weights = generator_model.state_dict()\n",
    "            best_d_weights = critic_model.state_dict()\n",
    "            best_g_loss = g_loss_hist[-1]\n",
    "\n",
    "        # Check how the generator is doing by saving G's output on fixed_noise\n",
    "        if (i % 24 == 0 or ((epoch == n_epochs - 1) and (i == len(dataloader) - 1))):\n",
    "            with torch.no_grad():\n",
    "                fake = generator_model(fixed_noise).detach().cpu()\n",
    "            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "plt.plot(d_loss_hist, label='d_loss')\n",
    "plt.plot(g_loss_hist, label='g_loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,8))\n",
    "plt.axis(\"off\")\n",
    "ims = [[plt.imshow(np.transpose(i,(1,2,0)), animated=True)] for i in img_list[-50:]]\n",
    "ani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\n",
    "\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models weights\n",
    "# model dir should be models/wgan_hr_mm_dd_mm\n",
    "model_dir = \"models/wgan_20_22_29_11\"\n",
    "torch.save(best_d_weights, f\"{model_dir}/discriminator.pth\")\n",
    "torch.save(best_g_weights, f\"{model_dir}/generator.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model dir should be models/dcgan_hr_mm_dd_mm\n",
    "model_dir = \"models/wgan_20_22_29_11\"\n",
    "\n",
    "# Load model and perform inference\n",
    "critic_model = Critic(use_gp=use_gp).to(device)\n",
    "critic_model.apply(weight_init)\n",
    "print(critic_model)\n",
    "\n",
    "generator_model = Generator().to(device)\n",
    "generator_model.apply(weight_init)\n",
    "print(generator_model)\n",
    "\n",
    "critic_model.load_state_dict(torch.load(f\"{model_dir}/discriminator.pth\"))\n",
    "generator_model.load_state_dict(torch.load(f\"{model_dir}/generator.pth\"))\n",
    "\n",
    "generator_model.eval()\n",
    "\n",
    "fixed_noise = torch.randn(16, latent_size, 1, 1, device=device)\n",
    "\n",
    "fake = generator_model(fixed_noise).detach().cpu()\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.imshow(\n",
    "    np.transpose(\n",
    "        vutils.make_grid(fake, padding=2, normalize=True),\n",
    "        (1, 2, 0)\n",
    "    )\n",
    ")\n",
    "plt.axis(\"off\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('vanilla')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e79bffa39ae3b34dd6a67cfb7a86efb7a981f71c16a69aea29c61b44b39f0d36"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
