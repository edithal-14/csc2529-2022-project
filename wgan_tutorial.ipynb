{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Critic model configuration\n",
    "clipping_param = 0.01\n",
    "\n",
    "# Training configuration\n",
    "device = torch.device(\"cuda:0\")\n",
    "batch_size = 32\n",
    "real_label = -1.0\n",
    "fake_label = 1.0\n",
    "n_epochs = 100\n",
    "n_critic = 5\n",
    "\n",
    "# Dataset configuration\n",
    "image_size = 28\n",
    "num_workers = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clip kernel weights between \n",
    "class ConstrainedConv2d(nn.Conv2d):\n",
    "    def forward(self, input):\n",
    "        return F.conv2d(\n",
    "            input,\n",
    "            self.weight.clamp(min=-clipping_param, max=clipping_param),\n",
    "            self.bias,\n",
    "            self.stride,\n",
    "            self.padding,\n",
    "            self.dilation,\n",
    "            self.groups\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_init_critic(m):\n",
    "    if isinstance(m, ConstrainedConv2d):\n",
    "        nn.init.normal_(m.weight, 0.0, 0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, in_shape=(28, 28, 1)) -> None:\n",
    "        super(Critic, self).__init__()\n",
    "        # input_shape = 28\n",
    "        self.input_shape = in_shape[0]\n",
    "        self.main = nn.Sequential(\n",
    "            # input: 28 x 28\n",
    "            ConstrainedConv2d(1, 1, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            # input: 14 x 14\n",
    "            ConstrainedConv2d(1, 1, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            # output: 7 x 7\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(\n",
    "                self.input_shape // 4 * self.input_shape // 4,\n",
    "                1\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Critic(\n",
       "  (main): Sequential(\n",
       "    (0): ConstrainedConv2d(1, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): LeakyReLU(negative_slope=0.2)\n",
       "    (3): ConstrainedConv2d(1, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (4): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): LeakyReLU(negative_slope=0.2)\n",
       "    (6): Flatten(start_dim=1, end_dim=-1)\n",
       "    (7): Linear(in_features=49, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "critic_model = Critic().to(device)\n",
    "critic_model.apply(weight_init_critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_init_generator(m):\n",
    "    if (\n",
    "        isinstance(m, nn.Linear) or\n",
    "        isinstance(m, nn.ConvTranspose2d) or\n",
    "        isinstance(m, nn.Conv2d)\n",
    "    ):\n",
    "        nn.init.normal_(m.weight, 0.0, 0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim) -> None:\n",
    "        super(Generator, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        # Lay foundation for 7 x 7 image with 128 channels\n",
    "        n_nodes = 128 * 7 * 7\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(latent_dim, n_nodes),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Unflatten(1, (128, 7, 7)),\n",
    "            # input: 7 x 7\n",
    "            nn.ConvTranspose2d(128, 128, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            # input: 14 x 14\n",
    "            nn.ConvTranspose2d(128, 1, 4, 2, 1),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            # input: 28 x 28\n",
    "            nn.Conv2d(1, 1, 7, padding=\"same\"),\n",
    "            nn.Tanh()\n",
    "            # output: 28 x 28\n",
    "        )\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (main): Sequential(\n",
       "    (0): Linear(in_features=50, out_features=6272, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.2)\n",
       "    (2): Unflatten(dim=1, unflattened_size=(128, 7, 7))\n",
       "    (3): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): LeakyReLU(negative_slope=0.2)\n",
       "    (6): ConvTranspose2d(128, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (7): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): LeakyReLU(negative_slope=0.2)\n",
       "    (9): Conv2d(1, 1, kernel_size=(7, 7), stride=(1, 1), padding=same)\n",
       "    (10): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator_model = Generator(50).to(device)\n",
    "generator_model.apply(weight_init_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torchvision.datasets.MNIST(\n",
    "    root=\"mnist_root_dir\",\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.CenterCrop(image_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(0.5, 0.5)\n",
    "    ])\n",
    ")\n",
    "filter_digit = 7\n",
    "idx = dataset.targets == filter_digit\n",
    "dataset.targets = dataset.targets[idx]\n",
    "dataset.data = dataset.data[idx]\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5, 27.5, 27.5, -0.5)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dummy_batch = next(iter(dataloader))\n",
    "plt.imshow(dummy_batch[0][23][0])\n",
    "plt.title(f\"Image for number: {dummy_batch[1][23]}\")\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_critic = optim.RMSprop(critic_model.parameters(), lr=5e-5)\n",
    "optimizer_generator = optim.RMSprop(generator_model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wasserstein_loss(y_true, y_pred):\n",
    "    return torch.mean(y_true * y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "print(\"Starting training....\")\n",
    "c1_hist = list()\n",
    "c2_hist = list()\n",
    "g_hist  = list()\n",
    "print(f\"Number of mini batch iterations per epoch: {len(dataloader)}\")\n",
    "for epoch in range(n_epochs):\n",
    "    for i, data in tqdm(enumerate(dataloader, 0)):\n",
    "        c1_loss = list()\n",
    "        c2_loss = list()\n",
    "        for _ in range(n_critic):\n",
    "            real_batch = data[0].to(device)\n",
    "            b_size = real_batch.shape[0]\n",
    "\n",
    "            # Update critic weight for real images\n",
    "            critic_model.zero_grad()\n",
    "            label = torch.full((b_size,), real_label, dtype=torch.float, device=device)\n",
    "            output = critic_model(real_batch)\n",
    "            loss = wasserstein_loss(label, output)\n",
    "            loss.backward()\n",
    "            c1_loss.append(loss.detach().cpu())\n",
    "            optimizer_critic.step()\n",
    "\n",
    "            # Update critic weight for fake images\n",
    "            critic_model.zero_grad()\n",
    "            noise = torch.randn(b_size, 50, device=device)\n",
    "            fake_batch = generator_model(noise)\n",
    "            label.fill_(fake_label)\n",
    "            output = critic_model(fake_batch)\n",
    "            loss = wasserstein_loss(label, output)\n",
    "            loss.backward()\n",
    "            c2_loss.append(loss.detach().cpu())\n",
    "            optimizer_critic.step()\n",
    "\n",
    "        c1_hist.append(np.mean(c1_loss))\n",
    "        c2_hist.append(np.mean(c2_loss))\n",
    "        # Update generator weight\n",
    "        generator_model.zero_grad()\n",
    "        critic_model.zero_grad()\n",
    "        noise = torch.randn(2 * b_size, 50, device=device)\n",
    "        fake_batch = generator_model(noise)\n",
    "        label.fill_(real_label)\n",
    "        output = critic_model(fake_batch)\n",
    "        loss = wasserstein_loss(label, output)\n",
    "        loss.backward()\n",
    "        g_hist.append(loss.detach().cpu())\n",
    "        optimizer_generator.step()\n",
    "\n",
    "    # Summarize performance every epoch\n",
    "    if not epoch % 1:\n",
    "        tqdm.write(\"\".join((\n",
    "            f\"epoch: {epoch}, c1 loss: {c1_hist[-1]:.4f}, \",\n",
    "            f\"c2 loss: {c2_hist[-1]:.4f}, g loss: {g_hist[-1]:.4f}\"\n",
    "        )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(d1_hist, d2_hist, g_hist):\n",
    "\t# plot history\n",
    "\tplt.plot(d1_hist, label='crit_real')\n",
    "\tplt.plot(d2_hist, label='crit_fake')\n",
    "\tplt.plot(g_hist, label='gen')\n",
    "\tplt.legend()\n",
    "\tplt.savefig('plot_line_plot_loss.png')\n",
    "\tplt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(c1_hist, c2_hist, g_hist)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('vanilla')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e79bffa39ae3b34dd6a67cfb7a86efb7a981f71c16a69aea29c61b44b39f0d36"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
